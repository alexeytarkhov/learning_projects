{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## токенизатор из предыдущих заданий\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pymorphy2\n",
    "\n",
    "\n",
    "MORPH = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "re_tokenizer = re.compile(r'[\\w]+')\n",
    "re_stopwordy = re.compile(r'[\\d_]+|[\\d]{2,}[\\w]+')\n",
    "morph_cache = {}\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    global morph_cache\n",
    "\n",
    "    s = s.lower().replace('ё', 'е')\n",
    "    tokens = []\n",
    "    for word in re_tokenizer.findall(s):\n",
    "        if re_stopwordy.match(word):\n",
    "            continue\n",
    "\n",
    "        wn = morph_cache.get(word, None)\n",
    "        \n",
    "        if wn == None:\n",
    "            p = MORPH.parse(word)[0]\n",
    "            wn = p.normal_form\n",
    "                \n",
    "            morph_cache[word] = wn\n",
    "\n",
    "        tokens.append(wn)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## загрузка обучающей выборки\n",
    "\n",
    "import os\n",
    "\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for filename in os.listdir('train/0'):\n",
    "    with open('train/0/' + filename, 'r') as collection_file:\n",
    "        x = collection_file.readlines()\n",
    "        train_texts.append(x[0])\n",
    "    train_labels.append('Политика')\n",
    "for filename in os.listdir('train/1'):\n",
    "    with open('train/1/' + filename, 'r') as collection_file:\n",
    "        x = collection_file.readlines()\n",
    "        train_texts.append(x[0])\n",
    "    train_labels.append('Технологии')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, X, y, alpha = 1):\n",
    "        self.alpha = alpha\n",
    "        self.classes = list(set(y))\n",
    "        self.vocabulary = set()\n",
    "        self.class_probs = dict()\n",
    "        self.word_probs = dict()\n",
    "        self.number_words = 0\n",
    "        docs = []\n",
    "        for text in X:\n",
    "            tokens = tokenize(text)\n",
    "            docs.append(tokens)\n",
    "            self.vocabulary.update(tokens)\n",
    "        self.vocabulary = list(self.vocabulary)\n",
    "        for word in self.vocabulary:\n",
    "            self.word_probs[word] = dict()\n",
    "        for n_class in self.classes:\n",
    "            class_docs = []\n",
    "            for i in range(len(y)):\n",
    "                if y[i] == n_class:\n",
    "                    class_docs.append(docs[i])\n",
    "            self.class_probs[n_class] = len(class_docs) / len(y)\n",
    "            number_words_in_class = 0\n",
    "            for doc in class_docs:\n",
    "                number_words_in_class += len(doc)\n",
    "            self.number_words += number_words_in_class\n",
    "            for word in self.vocabulary:\n",
    "                word_in_class_counter = 0\n",
    "                for doc in class_docs:\n",
    "                    word_in_class_counter += len(list(filter(lambda x: x == word, doc)))\n",
    "                self.word_probs[word][n_class] = (word_in_class_counter + self.alpha) / (number_words_in_class \n",
    "                                                                                         + self.alpha * len(self.vocabulary))\n",
    "                \n",
    "           \n",
    "    def classification(self, text):\n",
    "        probs = dict()\n",
    "        tokens = tokenize(text)\n",
    "        for n_class in self.classes:\n",
    "            probs[n_class] = log(self.class_probs[n_class])\n",
    "            for word in tokens:\n",
    "                if word in self.vocabulary:\n",
    "                    probs[n_class] += log(self.word_probs[word][n_class])\n",
    "                else:\n",
    "                    probs[n_class] += log(self.alpha / (self.number_words + self.alpha * len(self.vocabulary)))\n",
    "            print(\"Класс \", n_class, \": \", probs[n_class])\n",
    "        return self.classes[list(probs.values()).index(max(probs.values()))]\n",
    "            \n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Класс  Технологии :  -212.64333002704893\n",
      "Класс  Политика :  -215.69353806539553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Технологии'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## пример работы классификатора\n",
    "NB = NaiveBayes(train_texts, train_labels)\n",
    "sample_text = 'Сайт магазина электроники «М.Видео» опубликовал характеристики смартфона «Яндекс.Телефон» за три дня до релиза. Запись была удалена, но доступна в кэше. Согласно опубликованным данным, смартфон обладает 4 Гб оперативной памяти и 64 Гб внутренней.'\n",
    "NB.classification(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
